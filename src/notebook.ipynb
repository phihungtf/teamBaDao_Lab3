{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"collapsed_sections":["tChWFAUf8CAg","867SO8F24Enh","7dph1tTf4HkU","OOBcV-zg8IPv","1p8qOmwE8MU8"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Part 0: Setting up"],"metadata":{"id":"tChWFAUf8CAg"}},{"cell_type":"markdown","source":["## Install and start MongoDB"],"metadata":{"id":"867SO8F24Enh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"enbk0ZkjoxPO"},"outputs":[],"source":["!apt install -qq mongodb\n","!service mongodb start"]},{"cell_type":"markdown","source":["## Download dataset and push to our mongodb "],"metadata":{"id":"7dph1tTf4HkU"}},{"cell_type":"code","source":["# It's already the 21st century and people are very impatient, so they use Brotli for text and Zstd for everything else.\n","# Reference: https://github.com/google/brotli\n","!apt-get install -qq brotli"],"metadata":{"id":"cy-r-AQ2pZM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -q https://csc14118.github.io/thuoc_raw.json.br\n","!wget -q https://csc14118.github.io/gia_ke_khai_raw.json.br\n","!wget -q https://csc14118.github.io/movies_lang.json.br "],"metadata":{"id":"XNT5v0BkpMWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!brotli -d *.br"],"metadata":{"id":"qqVJXO4TpQDZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q pymongo"],"metadata":{"id":"p2AGMuFp6x2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from pymongo import MongoClient\n","\n","client = MongoClient()\n","\n","# Creation of the new database\n","db = client['input_data']\n","\n","collection_name = [\"gia_ke_khai_raw\", \"movies_lang\", \"thuoc_raw\"]\n","\n","# Push our data to mongodb\n","for data in collection_name:\n","    collection = db[data]\n","    chunks = json.load(open(f'{data}.json'))\n","    collection.insert_many(chunks)\n","\n","# Create a dummy database to test\n","db = client['dummy']\n","db['chunks'].insert_many([{'Banh xeo': 'Rat ngon'},{'Banh bao': 'Cung ngon'}])\n","\n","client.list_database_names()"],"metadata":{"id":"4lGAVCVCpz_9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install pyspark"],"metadata":{"id":"BPgvKHYi46Zh"}},{"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q \"https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\"\n","!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n","!pip install -q findspark"],"metadata":{"id":"3BQOC48fkpMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"spark-3.1.1-bin-hadoop2.7\"\n","import findspark\n","findspark.init()\n","findspark.find()"],"metadata":{"id":"nJFxkwtAkt3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyspark\n","print(pyspark.__version__)"],"metadata":{"id":"qgjKoGjDzZgu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dirty trick to connect spark to our mongodb\n","\n","In industry environment, please read the docs carefully to seting up these complicated things."],"metadata":{"id":"N4x3MtDq494s"}},{"cell_type":"code","source":["!rm $SPARK_HOME/jars/mongo*.jar\n","!rm $SPARK_HOME/jars/bson*.jar"],"metadata":{"id":"J3kZd3CZNIY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cd $SPARK_HOME/jars && wget https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.1.1/mongo-spark-connector_2.12-10.1.1.jar\n","!cd $SPARK_HOME/jars && wget https://repo1.maven.org/maven2/org/mongodb/mongodb-driver/3.12.12/mongodb-driver-3.12.12.jar\n","!cd $SPARK_HOME/jars && wget https://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/3.12.12/mongo-java-driver-3.12.12.jar\n","!cd $SPARK_HOME/jars && wget https://repo1.maven.org/maven2/org/mongodb/bson/4.6.0/bson-4.6.0.jar"],"metadata":{"id":"wPavC4NBR0fn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.shell import spark\n","from pyspark import SparkContext,SparkConf\n","\n","uri = \"mongodb://localhost:27017/input_data\"\n","\n","from pyspark.sql import SparkSession\n","\n","spark_jb = \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\"\n","my_spark = SparkSession \\\n","    .builder \\\n","    .config(\"spark.executor.memory\", \"1g\") \\\n","    .appName(\"csc14112\") \\\n","    .config(\"spark.mongodb.read.connection.uri\", uri) \\\n","    .config(\"spark.mongodb.write.connection.uri\", uri) \\\n","    .getOrCreate()"],"metadata":{"id":"E9Z415K4fRED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test read data from our mongo db\n","p = my_spark.read.format(\"mongodb\").option(\"database\",\"dummy\").option(\"collection\", \"chunk\").load()\n","p.printSchema()"],"metadata":{"id":"VIcRV6NEfdM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p.show()"],"metadata":{"id":"7QDbkF7Wpl4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 1: Introduction to PySpark"],"metadata":{"id":"OOBcV-zg8IPv"}},{"cell_type":"markdown","source":["\n","In this lab assignment, we will work with a movie dataset loaded into our MongoDB at `input_data.movies_lang`. We will use PySpark RDD and DataFrame to perform the following tasks:"],"metadata":{"id":"LejimkJO8u6A"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","# Read data from mongodb\n","\n","raise NotImplementedError"],"metadata":{"id":"29SbDUgzQksL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (a) Count the number of movies by country. Sort by count in decreasing order."],"metadata":{"id":"mjQyMcL59FwB"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"qOKzEl44nj6V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (b) Return the titles of the movies produced in France."],"metadata":{"id":"Gi9OiL_K8610"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"nuIk85yf-5EX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (c) Return the title of the movies of which Sofia Coppola is one of the actresses. "],"metadata":{"id":"RWIrwllI-9LI"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"XznCrQxy-858"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (d) Return the names and birth dates of the directors of movies produced in France.\n"],"metadata":{"id":"rqtvCzPV_Con"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"ZBzAdOj8_Cb3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (e) Return the average number of actors in a film.\n"],"metadata":{"id":"jtYnldOV_MXj"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"RQmY486U_PaW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (f) Return the name of the actor that acted in the most movies."],"metadata":{"id":"vbZmOOAz_RDj"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"V6ZKk9Ts_StS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 2: Real-world Data Manipulation"],"metadata":{"id":"1p8qOmwE8MU8"}},{"cell_type":"markdown","source":["In this part of the lab, we will work with two collections in our MongoDB: `gia_ke_khai_raw` and `thuoc_raw` loaded at `input_data.gia_ke_khai_raw` and `input_data.thuoc_raw` respectively. We will use PySpark RDD and DataFrame to perform the following tasks:"],"metadata":{"id":"6Xjjk5Sg_xuO"}},{"cell_type":"markdown","source":["### (a)  Read the datasets into a DataFrame and print out the schema and the number of records."],"metadata":{"id":"QJXKtUUkHDLn"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"BsEKB1yk_o7d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (b) Show all records in the `thuoc_raw` collection that have the same active pharmaceutical ingredient (API) in their `hoatChat` field as their medicine name.\n","\n","\n","Notes: In the context of medication, API stands for Active Pharmaceutical Ingredient, which is the biologically active component in a drug that produces the intended therapeutic effect. In other words, it is the chemical substance that gives a medicine its medicinal properties."],"metadata":{"id":"KrwAxI--bzks"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"myaTwNj8byW0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (c) Create a new DataFrame from the `thuoc_raw` collection that splits the API in the `hoatChat` field into multiple rows. For example, \"paracetamol\" is the API in \"Paracetamol 500 mg,\" and \"amoxicillin\" is the API in various medications such as \"Amogentine 500mg/125mg,\" \"Augbactam 1g/200mg,\" and \"Viamomentin.\" The resulting DataFrame should have two columns: `hoatChat` and `thuocTuongUng` as a list. After processing the data, write it back to our MongoDB at `output_data.thuocthaythe`."],"metadata":{"id":"9e0N9KtvIIS4"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"BQzlRdrKag1A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (d) Create new DataFrame from two mentioned above that contains  `tenThuoc`, `hoatChat`, `dongGoi`, `dvt` and `giaBan`. After process the data, write it back to our mongodb at `output_data.giathuoc`."],"metadata":{"id":"sQ00j4TuHosd"}},{"cell_type":"code","source":["# YOUR CODE HERE\n","\n","raise NotImplementedError"],"metadata":{"id":"O8YNrN4lAZa5"},"execution_count":null,"outputs":[]}]}